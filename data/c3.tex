%!TEX root = ../document.tex
\chapter{基于双向长短期记忆网络的序列感知推荐}

\section{问题的提出}
推荐系统面临的问题主要有两大类：评分预测和项目推荐。所以在推荐领域根据用户的历史活动记录预测%
用户下一次行为可能会选择什么项目也是一个重要的问题。在许多在线网站和应用程序当中，如在线电子%
商务、新闻或视频推荐网站、音乐或广播电台，它们都需要为用户提供一个杰出服务来推荐用户在未来可能%
会喜欢的东西。现有的推荐系统主要关注于找出用户或项目额近邻集，或者利用隐式或显式信息(如标签、%
评论、物品内容、用户属性)来提升近邻感知能力。然而，却少有工作利用数据当中的时序属性来直接构建推荐系统。%
在本篇论文中，我们发现数据的序列中其实包含着许多有价值的且激动人心的信息，以视频网站为例，一个用户看%
了纪录片《河西走廊》第一集《使者》之后，接下来看的另一个节目很有可能会是《河西走廊》第一集《通道》。甚至早在%
2011年举办的Recsys推荐系统大会上，来自音乐应用Pandora\footnote{\url{www.pandora.com}}的研究%
人员给出的演讲上都提到了许多用户听音乐具有时序特点。%

在某些特别的应用场景下，常规的推荐系统甚至无法起作用。现有的推荐系统都需要分析用户的数据，因此每个%
网站和应用的使用到需要让用户完成注册以及登录，然而用户每次使用网站或者应用的服务时都不一定会愿意%
登录，这种场景下对匿名用户的推荐显然挑战更大，常规的推荐策略显然无法起作用，基于匿名用户本地浏览器%
和缓存的会话所蕴含的序列进行推荐则显现出很重要的实践意义与价值。


%
%


\section{基于双向长短期记忆网络的序列感知推荐模型}
\subsection{目标问题定义}

序列感知推荐与传统的单类协同过滤推荐是有很大不同的，序列感知推荐的主要目标是预测用户下一步将会%
点击什么，而且利用数据仅仅包含用户当前历史行为的序列集合，而不接触为用户设置的长期偏好属性。%
接下来我们将定义序列感知这一问题的形式。%

在序列感知推荐当中，我们定义$\mathbb{U}= \left \{ u_{1},u_{2},...,u_{N} \right \}\label{eq}$%
代表不同的用户集合，定义$\mathbb{I}= \left \{ i_{1},i_{2},...,i_{M} \right \}$代表在所有序列%
中出现过的不同物品集合，$s_{u}^{t}\in \mathbb{I}$表示用户$u$在时间刻$t$点击某一个物品的记录，%
该记录对应的物品包含在物品集合$\mathbb{I}$当中。对于每一个用户$u$，%
都记录一个按照数据诞生时间戳顺序排列的用户点击记录%
序列$\mathbb{S}_{u}=\left \{ s_{u}^{1},s_{u}^{2},...,s_{u}^{t-1},s_{u}^{t} \right \}$%
序列感知推荐的目标是预测下一次点击行为，也就是做出$t+1$时刻的推荐$s_{u}^{t+1}$。在序列感知推荐%
模型当中，对于序列$s$，模型的输出是所有候选物品对象可能被点击的概率$\hat{y}$，而概率最大的$K$个%
输出所对应的候选物品将作为推荐项目给到用户。%
%
%
%
%

\subsection{嵌入层}

我们使用用户消费历史的最近几个序列当做特征，用户消费的最后一个物品当做标签，来构建一个超多分类%
的有监督学习模型。因此，在特征工程阶段，我们需要将原始序列特征数据转换为计算机容易处理的向量形式%
并且与标签映射。One\_hot编码是用来表达离散特征的最常用的向量表达形式，然而One\_hot编码的向量会遇%
到高维和稀疏的问题。如果我们使用One\_hot编码来处理具有1000个类别的特征，那么每个特征会被一个拥%
有1000个数字的向量来表示，但其中999个数字会是0.在一个大规模数据集中，就计算效率而言这种方式是%
不可取的。而词嵌入技术在自然语言处理领域大放异彩，我们可以借用词嵌入技术，也构造一个嵌入矩阵来%
得到比One\_hot形式小得多的向量结构：
$$
e(I_i) = EI_i
$$
其中$E\in \mathbb{R}^{|e|\times |M|}$, $|e|$是嵌入层的大小，$|M|$是训练集中不同项目的数量。%
所以$e(I_i)$是$I_i$的嵌入表达，其是一个具有$e$个实数的稠密矩阵。与一个 $|M|\times |M|$大小的%
One\_hot编码形式相比，我们的序列嵌入矩阵的大小为$|e|\times |M|$，当处理大数据集时，这显著减小%
了内存消耗。

\subsection{用户短期兴趣学习}

我们通过结合用户$u$在$t$时刻消费的项目$I_{u}^{t}$和其先后的项目来表达用户在此时刻的兴趣。行为序列帮助我们更精确地揭示了用户的短期兴趣。在这个推荐系统中，我们使用了一个双向的长短期记忆神经网络构建的循环结构来捕捉用户短期的兴趣变化。

我们定义$h_{b}(I_{i})$ 为用户他在消费物品$I_i$之前的兴趣，$h_{a}(I_{i})$为用户消费物品$I_i$之后的兴趣。$h_{b}(I_{i})$和$h_{a}(I_{i})$都是具有$|h|$个实数的稠密向量。$W^{(b)}$是隐藏层的权重矩阵，用来继承用户之前的兴趣状态，矩阵$W^{(cb)}$用来结合前一个物品的嵌入表达，$\sigma$是一个非线性激活函数，因此通过学习表达式$h_{b}(I_{i})$来学习用户消费物品$I_i$之前的兴趣。同理，用户消费物品$I_i$之后的兴趣通过学习表达式$h_{a}(I_{i})$来学习
。所有用户的初始兴趣使用同样的参数$h_{b}(I_{1})$，用户消费历史中的最后的兴趣则共享参数$h_{a}(I_{n})$
$$
h_{b}(I_{i})=\sigma (W^{(b)}h_{b}(I_{i-1})+W^{(cb)}e(I_{i-1}))
$$
$$
h_{a}(I_{i})=\sigma (W^{(a)}h_{a}(I_{i+1})+W^{(ca)}e(I_{i+1}))
$$

通过以上公式，学习用户某个时刻之前和之后的兴趣，将它们和用户当前消费物品的嵌入矩阵集合来表达此时刻用户的临时兴趣状态，其结合形式如下：
$$
x_{i}=[h_{b}(I_{i});e(I_{i});h_{a}(I_{i})]
$$
所以通过使用大量用户的历史行为序列$\left \{ i_{1},i_{2},...,i_{n-1},i_{n} \right \}$,如果我们的模型学习到了某个用户消费物品$i_{n-1}$时的临时兴趣$x_{n-1}$，他将更有可能得到一个物品推荐$i_{n}$.双向循环结构能够对序列的前向扫描中捕获所有的$h_b$，反向扫描则捕获了所有的$h_a$.当训练集中所有的临时兴趣状态$x_i$都被捕获之后，运用一个线性转换与tanh激活函数将结果送到下一层。

$$
y_{i}^{(2)}=tanh(W^{(2)}x_{i}+b^{(2)})
$$
$y_{i}^{(2)}$是潜在的兴趣向量，其中的每一个兴趣向量将通过上面权重和参数的更新来决定影响用户消费序列中最重要的因素。

\subsection{流行趋势学习}
当用户消费项目的所有序列被计算完之后，接下来应用一个最大池化层：
$$
y^{(3)}=\max_{i=1}^{n}y_{i}^{(2)}
$$
最大池化通过应用一个最大过滤器到上层代表的非重叠子区域，有了池化层，模型的参数或权重迅速减小了，这样也能减小上层输入的空间维度，减小计算消耗。通过对全局序列属性的捕获最大输出层能在用户的这个历史记录里找到那些最流行的序列组合。模型的最后一层就是常规的输出层了：
$$
y^{(4)}=W^{(4)}y^{(3)}+b^{(4)}
$$
输出层通过应用一个softmax激活函数到$y^{(4)}$来转换成下一个类别的输出概率：
$$
p_{i}= \frac{e^{y_{i}^{(4)}}}{\sum_{k=1}^{n}e^{y_{k}^{(4)}}}
$$



\section{复杂度分析}
文档由许多词语特征组成，关键词对文档具有一定表征能力，%
因此，从与用户检索匹配到的文献摘要中抽取出关键词，这些关键词%
在一定意义上用户学者检索的内容，另一角度也反应了论文学%
者的研究内容，因此，本文的关键词网络是基于词项共现(Term Co-ocurrence)构建的，核心思想是用词项之间的共现程度%
反映语义之间的联系，经过多种方式挖掘关键词网络，获得%
将用户学者和论文作者之间联系较为紧密的词项，以此得到%
了两者之间联系的桥梁。

在没有推荐系统的时期，一个科研学者想投身于一个全新的研%
究领域时，一开始他只简单的了解了该领域的概况，对于该领%
域的具体分支以及对哪个分支的研究内容感兴趣，因此，他会%
通过关键词检索，检索并查阅大量的文献，从而确定自己的研%
究内容。例如张三想研究自然语言处理(NLP)领域，但自然语言处理包含很多任务，如文本内部特征研究(词性标注、分词等)，文本分类、聚类，%
特征提取(命名实体识别、关键词抽取)，知识图谱应用等。%
首先他可能会检索“自然语言处理”，得到系列该领域的文献概述，%
通过大量阅读获取到各个分支领域的信息，张三感觉关键词抽取、%
命名实体识别两个话题比较感兴趣，他则会依次检索“关键词抽取”、%
“命名实习识别”获取相关的信息(如该领域研究现状、%
出色的学者及相关的文献)。总之，科研人员会通过反复检索%
关键词获取目标信息，以此开展后续的研究。因此本文将用户%
搜索的关键词与该关键词匹配的文献关键词进行关键词网络构建，%
该网络的功能主要是用来检测用户学者的搜索兴趣和明确检索意图，%
通过基于图的算法挖掘关键词网络，自动预测他可能会感兴趣的研%
究内容，进而也为后续给他推荐相似的学者奠定基础。

\subsection{构建关键词网络KCG}
构建截图，关键词网络的作用，数据节点和边数表%

本节主要介绍关键词网络$KCG$的构建过程，根据用户搜索的关键词，系统会匹配数据库里的数据，%
将包含搜索关键词的文献记录返回，通过抽取出文献的关键词，将关键词以共现的关系构建出关键词网络图%
，即$KCG$网络图。例如用户搜索关键词为$"HFMD"$，返回包含$"HFMD"$的文献摘要，动态抽取出每篇文献摘要的%
关键词，再将这些关键词以共现关系构建成$KCG$图（为了加快计算过程，实现快速推进的效果，本文实现先离线抽取出每篇文%
献摘要关键词，用户搜索后直接返回关键词集合），共现窗口大小$window$为摘要的长度。%
下表\ref{tab:KCG}为在四个数据集上分别构建的词共现图的节点数目和边数目详细信息，在手足口病数据集上的节点数为%
643，边数为7705，在癌症数据集上的节点数为164325，边数为4223234。可见在癌症的$KCG$网络图非常大。%



\subsection{挖掘KCG网络核心点}



\section{学者-关键词模型}

本文将学者推荐问题建模在二分图上，该二分图网络的节点包括被推荐的对象，即论文%
作者($Author\_node$)，和论文的关键词($Keyword\_node$)，如果$Keyword_i$出%
现在作者$Author_i$的论文中，则$Keyword_i$和$Author_i$存在一条边相连，%
该图是一个无向无权图，为了描述方便，本文将该二分图简称为AKG。通常情况下，推荐系统是在寻找用户%
Users集合中每个$user_i$和被推荐对象Authors集合中的$author_j$之间的关系,%
例如可以通过计算的方式给每对($user_i$，$author_j$)组合进行打分，这个分数就可以被用来衡量用户$user_i$%
可能对被推荐对象$author_j$感兴趣的程度，本文用兴趣即关键词作为联%
系$user_i$和$author_j$两者的桥梁。本文尝试采用PersonalRank算法对构建的AKG二分图中的节
点进行打分，最后将Top K个得分较高的论文作者作为推荐对象推荐给用户。%

\subsection{构建学者-关键词AKG网络}
通过上段的描述可知该$AKG$网络图的构建方式，用邻接矩阵$M=(M_{i,j})$来表示$AKG$的权值和边，%
即$M$定义如下~(\ref{equ:chap03:M}),本文通过模拟用户检索关键词的过程，%

\subsection{Top K推荐打分算法}
为了发现被推荐对象与用户之间关系，目前已经有很多文献提出的算法是基于图结构的数据上，%
例如Gori发表在IJCAJ会议上的一篇关于电影推荐的论文\upcite{gori2007itemrank}，%
Gori将电影推荐问题建模在图上，并且改进PageRank算法后提出ItemRank，采用ItemRank%
算法对图中的节点进行打分，本文借鉴其思想，采用PersonalRank算法对AKG网络图进行打分，%
最后给用户推荐分值高的前Top K个可能感兴趣的论文作者。%
PersonalRank\upcite{haveliwala2002topic}算法于2002年被Haveliwala提出，%
是一种基于随机游走的图算法，该算法与经典网页重要性排序PageRank算法非常相似，%
为网络中的每个节点计算重要性得分，并且文献\upcite{wu2018ga}表示其在网络图中表现出很好的性能。%
PersonalRank算法的计算公式如下~\ref{equ:chap03:pr}所示：
\begin{equation}
\label{equ:chap03:pr}
  \mathbf{PR(i)}=\frac{(1-\alpha)}{r_i}+\alpha\sum_{j\in
  in(i)}\frac{PR(j)}{|out(i)|}\quad
   r_i =
  \begin{cases}
   1\quad i=u \\
   0\quad i!=u
  \end{cases}
\end{equation}

\subsection{算法实现}
\begin{algorithm}[htbp]
	\caption{基于双向长短期记忆网络的序列感知推荐算法}
	\label{alg:bi-lstm}
		\begin{algorithmic}[1]
			\REQUIRE 长文本数据：$T$；最大语句数量：$maxNum$； 最大字数：$maxLen$
			\ENSURE 短文本数据：$Tokens$
			\STATE $sentences \leftarrow T$, 其中$sentences=\{sen_{1}, sen_{2}...sen_{n}\}$
			\IF {$n > maxNum$}
			  \FOR {$i=0$ to $n$}
			    \STATE 计算文本中每个语句的权重：$score_{sen_{i}}$
			  \ENDFOR
			  \STATE 根据$score$对语句有大到小排序
			  \STATE 选取TOP N语句：$sentences_{topn} \subseteqq sentences$
			  \STATE $words \leftarrow sentences_{topn}$，其中 $words=\{word_{1}, word_{2}...word_{m}\}$
			\ELSE
			  \STATE $words \leftarrow sentence$，其中 $words=\{word_{1}, word_{2}...word_{m}\}$
			\ENDIF
			\IF {$m > maxLen$}
			  \STATE $Tokens \subseteqq words$
			\ELSE
			  \STATE $Tokens = words $
			\ENDIF
			\RETURN $Tokens$
		\end{algorithmic}
\end{algorithm}

\section{本章小结}



