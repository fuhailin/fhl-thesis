%!TEX root = ../document.tex
\chapter{个性化推荐、序列感知推荐及迁移学习}
\section{推荐系统}
推荐系统（RS）已发展成为帮助用户做出明智决策和选择的基本工具，尤其是在大数据时代，%
客户必须从大量产品和服务中做出选择。因此现代推荐系统是在当前大数据环境下应运而生的，%
现代推荐系统的架构如图\ref{fig:structer}所示，本文讨论的问题主要出于推荐算法层面。


\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{RS_Architecture.pdf}\\
  \caption{现代推荐系统架构图}
  \label{fig:RS_Architecture}
\end{figure}
%
本章节主要从传统机器学习和深度学习两个方面介绍现代推荐系统中常用的各种推荐算法，%
从它们各自的特点分析现如今存在的问题。

\subsection{传统推荐算法}

传统的经典推荐算法主要可分为三大类，它们分别是基于协同过滤的推荐算法、基于内容的%
推荐算法和混合推荐算法。协同过滤算法大体上可以分为基于内存的协同过滤(Memory-based CF)%
和基于模型的协同过滤(Model-based CF)。基于内存的协同过滤通过为用户寻找相似用户%
或物品集合做推荐，所以可细分为基于用户的协同过滤%
(User-based Collaborative Filtering，UBCF)和基于物品的协同过滤%
(Item-based Collaborative Filtering，IBCF)。而基于模型的%
协同过滤则主要利用评分信息训练相应的模型，然后使用这个模型对未知数据进行预测，%
这类算法有贝叶斯网络\upcite{Chen02abayesian}、聚类模型\upcite{ungar1998clustering}、%
概率矩阵分解\upcite{NIPS2007_3208}等等。基于内容的推荐算法则基于物品的描述或者用户%
的属性描述做出推荐。混合推荐系统则将上述两大类的方法结合起来。基于传统推荐算法的特点，%
基于协同过滤的推荐算法中大多需要构造一个用户与物品的交互矩阵，随着大数据的极速发展，%
物品与用户的数量往往能达到上亿规模，使得交互矩阵构造的空间复杂度过高，而基于内容的%
推荐精确度往往有限，还面临着冷启动的问题，在大数据时代传统推荐算法已慢慢无法解决当%
前问题。

\subsection{基于深度学习的推荐算法}

得益于神经网络的反向传播理论，批量梯度下降的优化方式使得机器学习特别是深度学习%
能处理的数据规模没有理论上的数量限制，现代推荐算法也借助深度学习的技术蓬勃发展。%
%


\subsection{推荐系统存在的问题}

\textbf{冷启动问题}%

\textbf{噪音问题}%

\textbf{数据长尾问题}%


\section{序列感知模型}
序列感知模型是把数据根据时间日期排序之后，按照数据之间的时间先后顺序，发现时间上近邻的数据之间的隐藏关系或数据的周期性变化规律等与时间有关系的一类数据挖掘模型，数据挖掘领域又常称这类模型为时序模型。因此序列感知模型面对的数据必须包含时间戳或者数据的存储形式能够不丢失数据的诞生先后顺序。时序模型数据分析的目的就是为了挖掘出数据之间的内在时间规律，找到这种时间规律之后利用其归纳、类推、演绎未来的数据变化趋势，从而进行建模样本之外的数据预测。


\subsection{序列感知推荐任务及场景}

当待分析的数据具有固有的顺序性质，序列学习方法就会在这些应用领域中有用，比较常见的应用有如自然语言处理、语音识别、时间序列预测、DNA建模，以及作为本文工作的核心内容，序列感知推荐。


\subsection{序列感知推荐技术分类}

\textbf{频繁集挖掘}

\textbf{马尔可夫链}

\textbf{循环神经网络}

因为传统前馈深度神经网络(FNN)无法了解给定输入的上下文环境关系，循环神经网络(RNN)\cite{RNN1994}被发明的目的就是用来进行对可变长度的序列数据进行建模。循环神经网络与传统的FNN模型之间的主要区别在于组成网络的单元中存在内部隐藏状态，在一个序列建模步骤中的每个内部隐藏状态节点都接收来自上一个节点的输入，因此这可以用一个循环来表示，其结构如图\ref{fig:rnn}所示，隐藏状态层保留了过去序列编码的摘要，每当RNN呈现新的输入时，就会更新该隐藏层的状态。对于一个最简单的标准循环神经网络其通过以下形式来更新隐藏单元的状态$h$：
\begin{equation}
  \label{eq:rnn}
  h_{n} = f \left(Ux_{n} + Wh_{n-1} + b \right)
\end{equation}
其中$h_{n-1}$是第$n-1$层神经网络的向量化表示，$x_{n}$是传递给第$n$层的输入序列编码，$U,W$是该层包含的权重矩阵，$b$是该层向量的偏置。函数$f(\cdot)$为非线性转换函数，也称激活函数，常用的激活函数有Logistic Sigmoid函数$\sigma(\cdot )$，$tanh(\cdot)$，线性整流单元$ReLU(\cdot)$和一些它们的变体。经过以上隐藏层状态的更新之后，输出层的计算公式如下：
\begin{equation}
  \label{eq:rnn1}
  \hat{y}_{n} = g(Vh_{n})
\end{equation}
其中$\hat{y}_{n}$是$n$时刻的输出值，$V$是输出层的权重矩阵，$g(\cdot)$是输出层的激活函数。

\begin{figure}[htb]%更改
  \centering
  \includegraphics[width=12cm]{rnn.png}\\
  \caption{简单循环神经网络结构图}
  \label{fig:rnn}
\end{figure}

通过公式\ref{eq:rnn},\ref{eq:rnn1}可以明白，RNN的每一个时间步骤都会有一个新的输入，并且特定时间步骤的输出依赖于之前所有步骤的输入，这意味着时间步骤$N$时刻的损失函数的计算要回溯到时间步骤$1$，这一过程也称为\textit{基于时间的反向传播算法(backpropagation through time, BPTT)}\upcite{Sutskever:2013:TRN:2604780}。但是如果要处理的序列很长的话，经过多层的反向传播，BPTT会产生梯度消息或者梯度爆炸的问题，以至于无法从差的很远的时间步骤中感知上下文环境，使得RNN的训练变得非常麻烦，RNN这一明显的缺点也称为长期依赖问题。

\textbf{长短期记忆神经网络}

为了解决RNN的长期依赖问题，一些基于公式\ref{eq:rnn}的变形工作诞生，其中最广为人知的是长短期记忆网络(long short-term memory, LSTM)\upcite{LSTM1997}。LSTM通过精巧设计的记忆单元更换了RNN中的隐藏单元，其核心计算单元如图\ref{fig:lstm}所示。LSTM神经元内部通过精心设计的分别称为遗忘门、输⼊门、输出门的三个门结构来决定哪些信息更新到内部或者从内部去除。在遗忘门\eqref{forget_gate}当中，前一时间步的掩藏状态和当前时间步的输入经过Sigmoid函数非线性转换之后得到一个$[0,1]$之间的值，以表示需要遗忘信息的概率。在输入门\eqref{input_gate},\eqref{input_gate1}中，将当前时间步骤中的输入和前一时间步骤学习到的隐藏状态经过tanh激活函数的计算生成一些候选值，并通过Sigmoid函数的传递从候选值中选出一些进行更新。而输出门\eqref{output_gate}就决定了当前单元要输出哪些部分。LSTM通过如下的组合函数来更新隐藏单元的状态：
  \begin{align} 
  f_{t} &=\sigma(U_{f} x_{t}+W_{f}[h_{t-1}+ c_{t-1}] +b_{f}) \label{forget_gate}\\
  i_{t} &=\sigma(U_{i} x_{t}+W_{i}[h_{t-1}+ c_{t-1}]+b_{i}) \label{input_gate}\\  
  c_{t} &=f_{t} c_{t-1}+i_{t} \tanh (U_{c} x_{t}+W_{c} h_{t-1}+b_{c}) \label{input_gate1}\\ 
  o_{t} &=\sigma(U_{o} x_{t}+W_{o}[h_{t-1}+ c_{t}]+b_{o}) \label{output_gate}\\ 
  h_{t} &=o_{t} \tanh (c_{t}) 
  \end{align}
% \end{equation}

其中$\sigma(\cdot)$是Logistic Sigmoid激活函数；而$i$、$f$、$o$和$c$分别是输入门、遗忘门、输出门和隐藏层的激活向量；变量$b$表示偏置向量，例如$b_{f}$表示为遗忘门的偏置向量。$U$、$W$分别代表输入向量的权重和上一层输出的权重，例如在遗忘门中，$U_{f}$代表遗忘门输入向量$x_{t}$的权重，$W_{f}$代表上一个LSTM神经元输出$h_{t-1}$的权重。

\begin{figure}[htb]
  \centering
  \includegraphics[width=12cm]{lstm.png}\\
  \caption{长短期记忆网络神经元结构图}
  \label{fig:lstm}
\end{figure}

\textbf{门控循环单元Gated Recurrent Unit}

由于LSTM在循环神经单元中增加了三个门结构，与RNN相比，在一个神经元当中要完成更多的复杂计算。当使用更大的网络的时候，训练时间相比RNN也将显著增加。为了减少训练的时间复杂度并同时保留LSTM对长期依赖关系的记忆能力，2014年Cho等人提出门控循环单元(Gated Recurrent Unit, GRU)\upcite{GRU2014}。与LSTM相似，GRU使用门结构建模单元内部信息的流动，不同的是，GRU将LSTM三个门减少为两个。GRU使用更新门来决定是否遗忘上时刻的信息或者记忆此时刻新的外部输入信息，其功能相当于组合了LSTM 中的输入门与遗忘门。使用重置门来决定如何将新的输入信息与内部已有放入记忆相结合。在$t$时刻GRU单元中更新门的状态表达式为:
$$
z_{t}^{j}=\sigma\left(U_{z} \mathbf{x}_{t}+W_{z} \mathbf{h}_{t-1}\right)^{j}
$$
其中，$\mathbf{x}_{t}$为第$t$个时间步骤的输入向量，$\mathbf{h}_{t-1}$中保存的是上一个时间步骤的信息。$U$，$W$是更新门当中输入向量与上一时间步信息的权重矩阵。$t$时刻GRU单元中重置门为：
$$
r_{t}^{j}=\sigma\left(U_{r} \mathbf{x}_{t}+W_{r} \mathbf{h}_{t-1}\right)^{j}
$$
结合更新门与重置门，整个GRU神经元在$t$时间步的内部状态更新为：
$$
z_{t}^{j}=\sigma\left(U_{z} \mathbf{x}_{t}+U_{z} \mathbf{h}_{t-1}\right)^{j}
$$
其中$\tilde{h}_{t}^{j}$为候选信息，表示当前记忆的内容，其计算表达式为：
$$
h_{t}^{j}=\left(1-z_{t}^{j}\right) h_{t-1}^{j}+z_{t}^{j} \tilde{h}_{t}^{j}
$$
单个GRU神经元的总体结构如图\ref{fig:gru}所示。

\begin{figure}[htb]
  \centering
  \includegraphics[width=12cm]{gru.png}\\
  \caption{门控循环单元神经元结构图}
  \label{fig:gru}
\end{figure}

\textbf{双向循环神经网络}

在自然语言处理的实体识别技术中，双向循环神经网络(Bidirectional recurrent neural networks, BRNN)\upcite{Schuster1997BidirectionalRN}弥补了单项循环神经网络对于上下文感知能力的不足，因为单向RNN预测下一个单词时使用的只是此单次出现之前的信息，而BRNN则从两个方向获取信息，上下文感知能力也就更强了。BRNN将隐藏层分为两个部分，前向状态层$\stackrel{\rightarrow}{h}$和反向状态层$\stackrel{\leftarrow}{h}$，其输出层的输入由$\stackrel{\rightarrow}{h}$和$\stackrel{\leftarrow}{h}$堆叠而成，其迭代公式如下：
\begin{align} 
  \vec{h}_{t} &= \sigma(U_{\vec{h}} x_{t}+W_{\vec{h} \vec{h}} \vec{h}_{t-1}+b_{\vec{h}}) \label{forword}\\
\stackrel{\leftarrow}{h}_{t} &= \sigma(U_{\stackrel{\leftarrow}{h}} x_{t}+W_{\stackrel{\leftarrow}{h} \stackrel{\leftarrow}{h}} \stackrel{\leftarrow}{h}_{t+1}+b_{\stackrel{\leftarrow}{h}}) \label{backword}\\
y_{t} &= W_{\vec{h} y} \vec{h}_{t}+W_{\stackrel{\leftarrow}{h}y} \stackrel{\leftarrow}{h}_{t}+b_{y} \label{stack}
  \end{align}

BRNN的隐藏层结构如图\ref{fig:brnn}所示。
\begin{figure}[htb]
  \centering
  \includegraphics[width=12cm]{brnn.png}\\
  \caption{双向循环网络神经结构图}
  \label{fig:brnn}
\end{figure}



\subsection{序列感知推荐算法评价指标}



\section{图神经网络}

\subsubsection{迁移学习研究内容}


\subsubsection{迁移学习分类}

\subsubsection{迁移学习方法}

\subsubsection{迁移学习面临的问题}

\section{本章小结}

